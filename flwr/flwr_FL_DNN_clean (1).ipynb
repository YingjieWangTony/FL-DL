{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bANoPbqWGWRA",
        "outputId": "4c7ab4c7-8083-4110-b9df-314f0ecbf446"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 KB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.5/90.5 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 KB\u001b[0m \u001b[31m788.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.2/280.2 KB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 KB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.48.2 requires grpcio>=1.48.2, but you have grpcio 1.43.0 which is incompatible.\n",
            "google-cloud-bigquery 3.4.1 requires grpcio<2.0dev,>=1.47.0, but you have grpcio 1.43.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q flwr[simulation] torch torchvision matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ043aExGXXo",
        "outputId": "bfeec551-378f-4662-eb75-0e53cf378659"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cpu using PyTorch 1.13.0+cu116 and Flower 1.1.0\n"
          ]
        }
      ],
      "source": [
        "from collections import OrderedDict\n",
        "from typing import List, Tuple\n",
        "\n",
        "import flwr as fl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from flwr.common import Metrics\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
        "print(f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLae2PQzLd-6"
      },
      "source": [
        "## Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEGaHFgxWErx"
      },
      "outputs": [],
      "source": [
        "# parameters\n",
        "time_step = 48\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_GLqxcrFe-B"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "with open ('House_30.txt', 'r') as reader:\n",
        "  for line in reader:\n",
        "    stripped_line = line.strip().split()\n",
        "    data.append(stripped_line)\n",
        "\n",
        "tem = [x[0] for x in data]\n",
        "houses = list(set(tem))\n",
        "\n",
        "date = []\n",
        "consumption = []\n",
        "for i in houses:\n",
        "  date.append([float(x[1]) for x in data if x[0]==i])\n",
        "  consumption.append([float(x[2]) for x in data if x[0]==i])    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFh5oiv0QHXG"
      },
      "outputs": [],
      "source": [
        "def create_label(data, time_step):\n",
        "  x_nest, y_nest = [], []\n",
        "  for j in range(len(data)):\n",
        "    x_data, y_data = [], []\n",
        "    for i in range(len(data[j]) - time_step):\n",
        "      x = data[j][i: (i + time_step)]\n",
        "      x_data.append(x)\n",
        "      y = [data[j][i + time_step]]\n",
        "      y_data.append(y)\n",
        "\n",
        "    #x_data = np.array(x_data)[:, :, np.newaxis]\n",
        "    x_data = np.array(x_data)[:, :]\n",
        "    #x_data = np.array(x_data)[:, np.newaxis, :]\n",
        "    x_nest.append(x_data)\n",
        "    y_nest.append(y_data)\n",
        "  x_nest = np.array(x_nest)\n",
        "  y_nest = np.array(y_nest)\n",
        "  return x_nest, y_nest\n",
        "# 可能要去掉x的最后一个维度 从（48，1）变为（48）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E50GczgkSAoF"
      },
      "outputs": [],
      "source": [
        "input, labels = create_label(consumption, time_step)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUlLivPStSy0"
      },
      "outputs": [],
      "source": [
        "input = np.float32(input)\n",
        "labels = np.float32(labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN0xa70WooMg"
      },
      "source": [
        "### Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55XqPaqDiGq1"
      },
      "outputs": [],
      "source": [
        "# 定义GetLoader类，继承Dataset方法，并重写__getitem__()和__len__()方法\n",
        "class GetLoader(torch.utils.data.Dataset):\n",
        "\t# 初始化函数，得到数据\n",
        "    def __init__(self, data_root, data_label):\n",
        "        self.data = data_root\n",
        "        self.label = data_label\n",
        "    # index是根据batchsize划分数据后得到的索引，最后将data和对应的labels进行一起返回\n",
        "    def __getitem__(self, index):\n",
        "        data = self.data[index]\n",
        "        labels = self.label[index]\n",
        "        return data, labels\n",
        "    # 该函数返回数据大小长度，目的是DataLoader方便划分，如果不知道大小，DataLoader会一脸懵逼\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpQp0W7zxZTP"
      },
      "outputs": [],
      "source": [
        "length = len(input[0])\n",
        "val = int(0.7*length)\n",
        "test = int(0.9*length)\n",
        "trainloaders = []\n",
        "valloaders = []\n",
        "testloaders = []\n",
        "\n",
        "def load_datasets(input, labels):\n",
        "\n",
        "  Xtrain_raw = [x[0: val] for x in input]\n",
        "  Xval_raw = [x[val: test] for x in input]\n",
        "  Xtest_raw = [x[test: ] for x in input]\n",
        "\n",
        "  Ytrain_raw = [x[0: val] for x in labels]\n",
        "  Yval_raw = [x[val: test] for x in labels]\n",
        "  Ytest_raw = [x[test: ] for x in labels]\n",
        "\n",
        "  for i in range(30):\n",
        "    ds_train = GetLoader(Xtrain_raw[i], Ytrain_raw[i])\n",
        "    trainloaders.append(DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True))\n",
        "    ds_val = GetLoader(Xval_raw[i], Yval_raw[i])\n",
        "    valloaders.append(DataLoader(ds_val, batch_size=BATCH_SIZE))\n",
        "    ds_test= GetLoader(Xtest_raw[i], Ytest_raw[i])\n",
        "    testloaders.append(DataLoader(ds_test, batch_size=BATCH_SIZE))\n",
        "\n",
        "  return trainloaders, valloaders, testloaders\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgeKNAg80aTF"
      },
      "outputs": [],
      "source": [
        "trainloaders, valloaders, testloaders = load_datasets(input, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVfo0GkH1fGd"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlFemn7iWd8m"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chi_F7TF1hxn"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        act = nn.Sigmoid\n",
        "        self.body = nn.Sequential(\n",
        "            nn.Linear(48, 48),\n",
        "            act(),\n",
        "            nn.Linear(48, 48),\n",
        "            act(),\n",
        "            nn.Linear(48, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        out = self.body(x)\n",
        "        return out\n",
        "\n",
        "net = Net().to(DEVICE)\n",
        "\n",
        "\n",
        "criterion = nn.MSELoss() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-mYJ2P8WIUe"
      },
      "outputs": [],
      "source": [
        "def train(net, trainloader, epochs: int, verbose=False):\n",
        "    \"\"\"Train the network on the training set.\"\"\"\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters())\n",
        "    net.train()\n",
        "    for epoch in range(epochs):\n",
        "        correct, total, epoch_loss = 0, 0, 0.0\n",
        "        for x, y in trainloader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(x)\n",
        "            loss = criterion(net(x), y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss\n",
        "           \n",
        "        epoch_loss /= len(trainloader.dataset)\n",
        "        if verbose:\n",
        "            print(f\"Epoch {epoch+1}: train loss {epoch_loss}\") \n",
        "\n",
        "\n",
        "def test(net, testloader):\n",
        "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
        "    criterion = nn.MSELoss()\n",
        "    correct, total, loss = 0, 0, 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in testloader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            outputs = net(x)\n",
        "            loss += criterion(outputs, y).item()\n",
        "\n",
        "    loss /= len(testloader.dataset)\n",
        "   \n",
        "    return loss "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB6uIeGTgLMn"
      },
      "source": [
        "### central test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnoFs1KRbUzT"
      },
      "outputs": [],
      "source": [
        "x, y = next(iter(trainloaders[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVJaWgOLXgpe",
        "outputId": "6326eb0c-7792-4c15-bc88-c09965cff22a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: validation loss 0.0005272865817465782\n",
            "Epoch 2: validation loss 0.0005215112082921317\n",
            "Epoch 3: validation loss 0.0004672112590357708\n",
            "Epoch 4: validation loss 0.000467944144985038\n",
            "Epoch 5: validation loss 0.00046927032099544966\n",
            "Final test set performance:\n",
            "\tloss 0.00042405893019403437\n"
          ]
        }
      ],
      "source": [
        "trainloader = trainloaders[0]\n",
        "valloader = valloaders[0]\n",
        "testloader = testloaders[0]\n",
        "net = Net().to(DEVICE)\n",
        "\n",
        "\n",
        "for epoch in range(5):\n",
        "    train(net, trainloader, 1)\n",
        "    loss = test(net, valloader)\n",
        "    print(f\"Epoch {epoch+1}: validation loss {loss}\")\n",
        "\n",
        "loss = test(net, testloader)\n",
        "print(f\"Final test set performance:\\n\\tloss {loss}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVu1Sc81gJA-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2YggcpRgPB-"
      },
      "source": [
        "## FL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1p2DJnRgRAW"
      },
      "outputs": [],
      "source": [
        "def get_parameters(net) -> List[np.ndarray]:\n",
        "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
        "\n",
        "def set_parameters(net, parameters: List[np.ndarray]):\n",
        "    params_dict = zip(net.state_dict().keys(), parameters)\n",
        "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "    net.load_state_dict(state_dict, strict=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez9P_QZLhiu1"
      },
      "outputs": [],
      "source": [
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, net, trainloader, valloader):\n",
        "        self.net = net\n",
        "        self.trainloader = trainloader\n",
        "        self.valloader = valloader\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return get_parameters(self.net)\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        set_parameters(self.net, parameters)\n",
        "        train(self.net, self.trainloader, epochs=1)\n",
        "        return get_parameters(self.net), len(self.trainloader), {}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        set_parameters(self.net, parameters)\n",
        "        loss = test(self.net, self.valloader)\n",
        "        return float(loss), len(self.valloader), {\"accuracy\": float(0)}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_TsTweQim4j"
      },
      "outputs": [],
      "source": [
        "def client_fn(cid: str) -> FlowerClient:\n",
        "    \"\"\"Create a Flower client representing a single organization.\"\"\"\n",
        "\n",
        "    # Load model\n",
        "    net = Net().to(DEVICE)\n",
        "\n",
        "\n",
        "\n",
        "    trainloader = trainloaders[int(cid)]\n",
        "    valloader = valloaders[int(cid)]\n",
        "\n",
        "    # Create a  single Flower client representing a single organization\n",
        "    return FlowerClient(net, trainloader, valloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RO_7kHysjbAL"
      },
      "outputs": [],
      "source": [
        "NUM_CLIENTS = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CCC8WGbjNLy",
        "outputId": "9790e9f0-a881-473e-a9c7-7e0a88c1ed92"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO flower 2023-01-13 15:49:57,209 | app.py:140 | Starting Flower simulation, config: ServerConfig(num_rounds=5, round_timeout=None)\n",
            "INFO:flower:Starting Flower simulation, config: ServerConfig(num_rounds=5, round_timeout=None)\n",
            "2023-01-13 15:49:59,925\tINFO worker.py:1518 -- Started a local Ray instance.\n",
            "INFO flower 2023-01-13 15:50:01,494 | app.py:174 | Flower VCE: Ray initialized with resources: {'node:172.28.0.2': 1.0, 'memory': 7535598798.0, 'object_store_memory': 3767799398.0, 'CPU': 2.0}\n",
            "INFO:flower:Flower VCE: Ray initialized with resources: {'node:172.28.0.2': 1.0, 'memory': 7535598798.0, 'object_store_memory': 3767799398.0, 'CPU': 2.0}\n",
            "INFO flower 2023-01-13 15:50:01,506 | server.py:86 | Initializing global parameters\n",
            "INFO:flower:Initializing global parameters\n",
            "INFO flower 2023-01-13 15:50:01,514 | server.py:270 | Requesting initial parameters from one random client\n",
            "INFO:flower:Requesting initial parameters from one random client\n",
            "INFO flower 2023-01-13 15:50:03,322 | server.py:274 | Received initial parameters from one random client\n",
            "INFO:flower:Received initial parameters from one random client\n",
            "INFO flower 2023-01-13 15:50:03,328 | server.py:88 | Evaluating initial parameters\n",
            "INFO:flower:Evaluating initial parameters\n",
            "INFO flower 2023-01-13 15:50:03,333 | server.py:101 | FL starting\n",
            "INFO:flower:FL starting\n",
            "DEBUG flower 2023-01-13 15:50:03,344 | server.py:215 | fit_round 1: strategy sampled 30 clients (out of 30)\n",
            "DEBUG:flower:fit_round 1: strategy sampled 30 clients (out of 30)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2365)\u001b[0m /usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py:172: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2365)\u001b[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2366)\u001b[0m /usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py:172: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2366)\u001b[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n",
            "DEBUG flower 2023-01-13 15:50:28,011 | server.py:229 | fit_round 1 received 30 results and 0 failures\n",
            "DEBUG:flower:fit_round 1 received 30 results and 0 failures\n",
            "WARNING flower 2023-01-13 15:50:28,070 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
            "WARNING:flower:No fit_metrics_aggregation_fn provided\n",
            "DEBUG flower 2023-01-13 15:50:28,076 | server.py:165 | evaluate_round 1: strategy sampled 9 clients (out of 30)\n",
            "DEBUG:flower:evaluate_round 1: strategy sampled 9 clients (out of 30)\n",
            "DEBUG flower 2023-01-13 15:50:29,170 | server.py:179 | evaluate_round 1 received 9 results and 0 failures\n",
            "DEBUG:flower:evaluate_round 1 received 9 results and 0 failures\n",
            "WARNING flower 2023-01-13 15:50:29,176 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided\n",
            "WARNING:flower:No evaluate_metrics_aggregation_fn provided\n",
            "DEBUG flower 2023-01-13 15:50:29,185 | server.py:215 | fit_round 2: strategy sampled 30 clients (out of 30)\n",
            "DEBUG:flower:fit_round 2: strategy sampled 30 clients (out of 30)\n",
            "DEBUG flower 2023-01-13 15:50:54,828 | server.py:229 | fit_round 2 received 30 results and 0 failures\n",
            "DEBUG:flower:fit_round 2 received 30 results and 0 failures\n",
            "DEBUG flower 2023-01-13 15:50:54,879 | server.py:165 | evaluate_round 2: strategy sampled 9 clients (out of 30)\n",
            "DEBUG:flower:evaluate_round 2: strategy sampled 9 clients (out of 30)\n",
            "DEBUG flower 2023-01-13 15:50:56,389 | server.py:179 | evaluate_round 2 received 9 results and 0 failures\n",
            "DEBUG:flower:evaluate_round 2 received 9 results and 0 failures\n",
            "DEBUG flower 2023-01-13 15:50:56,392 | server.py:215 | fit_round 3: strategy sampled 30 clients (out of 30)\n",
            "DEBUG:flower:fit_round 3: strategy sampled 30 clients (out of 30)\n",
            "DEBUG flower 2023-01-13 15:51:22,874 | server.py:229 | fit_round 3 received 30 results and 0 failures\n",
            "DEBUG:flower:fit_round 3 received 30 results and 0 failures\n",
            "DEBUG flower 2023-01-13 15:51:22,959 | server.py:165 | evaluate_round 3: strategy sampled 9 clients (out of 30)\n",
            "DEBUG:flower:evaluate_round 3: strategy sampled 9 clients (out of 30)\n",
            "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 2203 MiB, 17 objects, write throughput 180 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n",
            "DEBUG flower 2023-01-13 15:51:27,783 | server.py:179 | evaluate_round 3 received 9 results and 0 failures\n",
            "DEBUG:flower:evaluate_round 3 received 9 results and 0 failures\n",
            "DEBUG flower 2023-01-13 15:51:27,793 | server.py:215 | fit_round 4: strategy sampled 30 clients (out of 30)\n",
            "DEBUG:flower:fit_round 4: strategy sampled 30 clients (out of 30)\n",
            "DEBUG flower 2023-01-13 15:51:50,630 | server.py:229 | fit_round 4 received 30 results and 0 failures\n",
            "DEBUG:flower:fit_round 4 received 30 results and 0 failures\n",
            "DEBUG flower 2023-01-13 15:51:50,690 | server.py:165 | evaluate_round 4: strategy sampled 9 clients (out of 30)\n",
            "DEBUG:flower:evaluate_round 4: strategy sampled 9 clients (out of 30)\n",
            "DEBUG flower 2023-01-13 15:51:52,757 | server.py:179 | evaluate_round 4 received 9 results and 0 failures\n",
            "DEBUG:flower:evaluate_round 4 received 9 results and 0 failures\n",
            "DEBUG flower 2023-01-13 15:51:52,763 | server.py:215 | fit_round 5: strategy sampled 30 clients (out of 30)\n",
            "DEBUG:flower:fit_round 5: strategy sampled 30 clients (out of 30)\n",
            "DEBUG flower 2023-01-13 15:52:16,902 | server.py:229 | fit_round 5 received 30 results and 0 failures\n",
            "DEBUG:flower:fit_round 5 received 30 results and 0 failures\n",
            "DEBUG flower 2023-01-13 15:52:16,965 | server.py:165 | evaluate_round 5: strategy sampled 9 clients (out of 30)\n",
            "DEBUG:flower:evaluate_round 5: strategy sampled 9 clients (out of 30)\n",
            "DEBUG flower 2023-01-13 15:52:18,066 | server.py:179 | evaluate_round 5 received 9 results and 0 failures\n",
            "DEBUG:flower:evaluate_round 5 received 9 results and 0 failures\n",
            "INFO flower 2023-01-13 15:52:18,070 | server.py:144 | FL finished in 134.72561449\n",
            "INFO:flower:FL finished in 134.72561449\n",
            "INFO flower 2023-01-13 15:52:18,075 | app.py:192 | app_fit: losses_distributed [(1, 0.00047746858803445803), (2, 0.0007220059671171081), (3, 0.0007651691735494773), (4, 0.0003658641672275786), (5, 0.0005834240170316223)]\n",
            "INFO:flower:app_fit: losses_distributed [(1, 0.00047746858803445803), (2, 0.0007220059671171081), (3, 0.0007651691735494773), (4, 0.0003658641672275786), (5, 0.0005834240170316223)]\n",
            "INFO flower 2023-01-13 15:52:18,078 | app.py:193 | app_fit: metrics_distributed {}\n",
            "INFO:flower:app_fit: metrics_distributed {}\n",
            "INFO flower 2023-01-13 15:52:18,080 | app.py:194 | app_fit: losses_centralized []\n",
            "INFO:flower:app_fit: losses_centralized []\n",
            "INFO flower 2023-01-13 15:52:18,083 | app.py:195 | app_fit: metrics_centralized {}\n",
            "INFO:flower:app_fit: metrics_centralized {}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "History (loss, distributed):\n",
              "\tround 1: 0.00047746858803445803\n",
              "\tround 2: 0.0007220059671171081\n",
              "\tround 3: 0.0007651691735494773\n",
              "\tround 4: 0.0003658641672275786\n",
              "\tround 5: 0.0005834240170316223"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create FedAvg strategy\n",
        "strategy = fl.server.strategy.FedAvg(\n",
        "        fraction_fit=1.0,  # Sample 100% of available clients for training\n",
        "        fraction_evaluate=0.3,  # Sample 50% of available clients for evaluation\n",
        "        min_fit_clients=10,  # Never sample less than 10 clients for training\n",
        "        min_evaluate_clients=5,  # Never sample less than 5 clients for evaluation\n",
        "        min_available_clients=10,  # Wait until all 10 clients are available\n",
        ")\n",
        "\n",
        "# Specify client resources if you need GPU (defaults to 1 CPU and 0 GPU)\n",
        "client_resources = None\n",
        "if DEVICE.type == \"cuda\":\n",
        "  client_resources = {\"num_gpus\": 1}\n",
        "\n",
        "# Start simulation\n",
        "fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=5), #10\n",
        "    strategy=strategy,\n",
        "    client_resources=client_resources,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
