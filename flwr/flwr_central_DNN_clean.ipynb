{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bANoPbqWGWRA",
        "outputId": "31fe360f-4684-450e-e870-71414f5f63e3"
      },
      "outputs": [],
      "source": [
        "!pip install -q flwr[simulation] torch torchvision matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ043aExGXXo",
        "outputId": "7cb0548c-f17f-4cc2-ee1f-4788b65940c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cpu using PyTorch 1.13.1+cu117 and Flower 1.2.0\n"
          ]
        }
      ],
      "source": [
        "from collections import OrderedDict\n",
        "from typing import List, Tuple\n",
        "\n",
        "import flwr as fl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from flwr.common import Metrics\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
        "print(f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLae2PQzLd-6"
      },
      "source": [
        "## Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aEGaHFgxWErx"
      },
      "outputs": [],
      "source": [
        "# parameters\n",
        "time_step = 48\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6_GLqxcrFe-B"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "with open ('House_30.txt', 'r') as reader:\n",
        "  for line in reader:\n",
        "    stripped_line = line.strip().split()\n",
        "    data.append(stripped_line)\n",
        "\n",
        "tem = [x[0] for x in data]\n",
        "houses = list(set(tem))\n",
        "\n",
        "date = []\n",
        "consumption = []\n",
        "for i in houses:\n",
        "  date.append([float(x[1]) for x in data if x[0]==i])\n",
        "  consumption.append([float(x[2]) for x in data if x[0]==i])    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lFh5oiv0QHXG"
      },
      "outputs": [],
      "source": [
        "def create_label(data, time_step):\n",
        "  x_nest, y_nest = [], []\n",
        "  for j in range(len(data)):\n",
        "    x_data, y_data = [], []\n",
        "    for i in range(len(data[j]) - time_step):\n",
        "      x = data[j][i: (i + time_step)]\n",
        "      x_data.append(x)\n",
        "      y = [data[j][i + time_step]]\n",
        "      y_data.append(y)\n",
        "\n",
        "    #x_data = np.array(x_data)[:, :, np.newaxis]\n",
        "    x_data = np.array(x_data)[:, :]\n",
        "    #x_data = np.array(x_data)[:, np.newaxis, :]\n",
        "    x_nest.append(x_data)\n",
        "    y_nest.append(y_data)\n",
        "  x_nest = np.array(x_nest)\n",
        "  y_nest = np.array(y_nest)\n",
        "  return x_nest, y_nest\n",
        "# 可能要去掉x的最后一个维度 从（48，1）变为（48）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "E50GczgkSAoF"
      },
      "outputs": [],
      "source": [
        "input, labels = create_label(consumption, time_step)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YUlLivPStSy0"
      },
      "outputs": [],
      "source": [
        "input = np.float32(input)\n",
        "labels = np.float32(labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN0xa70WooMg"
      },
      "source": [
        "### Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "55XqPaqDiGq1"
      },
      "outputs": [],
      "source": [
        "# 定义GetLoader类，继承Dataset方法，并重写__getitem__()和__len__()方法\n",
        "class GetLoader(torch.utils.data.Dataset):\n",
        "\t# 初始化函数，得到数据\n",
        "    def __init__(self, data_root, data_label):\n",
        "        self.data = data_root\n",
        "        self.label = data_label\n",
        "    # index是根据batchsize划分数据后得到的索引，最后将data和对应的labels进行一起返回\n",
        "    def __getitem__(self, index):\n",
        "        data = self.data[index]\n",
        "        labels = self.label[index]\n",
        "        return data, labels\n",
        "    # 该函数返回数据大小长度，目的是DataLoader方便划分，如果不知道大小，DataLoader会一脸懵逼\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EpQp0W7zxZTP"
      },
      "outputs": [],
      "source": [
        "length = len(input[0])\n",
        "val = int(0.7*length)\n",
        "test = int(0.9*length)\n",
        "trainloaders = []\n",
        "valloaders = []\n",
        "testloaders = []\n",
        "\n",
        "def load_datasets(input, labels):\n",
        "\n",
        "  Xtrain_raw = [x[0: val] for x in input]\n",
        "  Xval_raw = [x[val: test] for x in input]\n",
        "  Xtest_raw = [x[test: ] for x in input]\n",
        "\n",
        "  Ytrain_raw = [x[0: val] for x in labels]\n",
        "  Yval_raw = [x[val: test] for x in labels]\n",
        "  Ytest_raw = [x[test: ] for x in labels]\n",
        "\n",
        "  for i in range(30):\n",
        "    ds_train = GetLoader(Xtrain_raw[i], Ytrain_raw[i])\n",
        "    trainloaders.append(DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True))\n",
        "    ds_val = GetLoader(Xval_raw[i], Yval_raw[i])\n",
        "    valloaders.append(DataLoader(ds_val, batch_size=BATCH_SIZE, drop_last=True))\n",
        "    ds_test= GetLoader(Xtest_raw[i], Ytest_raw[i])\n",
        "    testloaders.append(DataLoader(ds_test, batch_size=BATCH_SIZE, drop_last=True))\n",
        "\n",
        "  return trainloaders, valloaders, testloaders\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xgeKNAg80aTF"
      },
      "outputs": [],
      "source": [
        "trainloaders, valloaders, testloaders = load_datasets(input, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVfo0GkH1fGd"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KlFemn7iWd8m"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "chi_F7TF1hxn"
      },
      "outputs": [],
      "source": [
        "class DNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DNN, self).__init__()\n",
        "        act = nn.Sigmoid\n",
        "        self.body = nn.Sequential(\n",
        "            nn.Linear(48, 48),\n",
        "            act(),\n",
        "            nn.Linear(48, 48),\n",
        "            act(),\n",
        "            nn.Linear(48, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        out = self.body(x)\n",
        "        return out\n",
        "\n",
        "criterion = nn.MSELoss() \n",
        "net = DNN().to(DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "b-mYJ2P8WIUe"
      },
      "outputs": [],
      "source": [
        "def train(net, trainloader, epochs: int, verbose=False):\n",
        "    \"\"\"Train the network on the training set.\"\"\"\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters())\n",
        "    net.train()\n",
        "    for epoch in range(epochs):\n",
        "        correct, total, epoch_loss = 0, 0, 0.0\n",
        "        for x, y in trainloader:\n",
        "            \n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #x = x.unsqueeze(0)\n",
        "            outputs = net(x)\n",
        "            loss = criterion(net(x), y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss\n",
        "           \n",
        "        epoch_loss /= len(trainloader.dataset)\n",
        "        if verbose:\n",
        "            print(f\"Epoch {epoch+1}: train loss {epoch_loss}\") \n",
        "\n",
        "\n",
        "def test(net, testloader):\n",
        "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
        "    criterion = nn.MSELoss()\n",
        "    correct, total, loss = 0, 0, 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in testloader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            outputs = net(x)\n",
        "            loss += criterion(outputs, y).item()\n",
        "\n",
        "    loss /= len(testloader.dataset)\n",
        "   \n",
        "    return loss "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB6uIeGTgLMn"
      },
      "source": [
        "### central test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BnoFs1KRbUzT"
      },
      "outputs": [],
      "source": [
        "x, y = next(iter(trainloaders[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XldM5rm0X52f",
        "outputId": "7811cf53-d790-4516-8caa-d9d7ec1c3b99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "client 0: centralized model loss 0.001308749587187323\n",
            "client 1: centralized model loss 0.0013043681227433788\n",
            "client 2: centralized model loss 0.0008208717107468189\n",
            "client 3: centralized model loss 0.00046298728983434396\n",
            "client 4: centralized model loss 0.000538724139325518\n",
            "client 5: centralized model loss 0.000885935108223785\n",
            "client 6: centralized model loss 0.0010877334485751666\n",
            "client 7: centralized model loss 0.0016185651102460154\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m trainloader \u001b[39m=\u001b[39m trainloaders[i]\n\u001b[1;32m      8\u001b[0m testloader \u001b[39m=\u001b[39m testloaders[i]\n\u001b[0;32m----> 9\u001b[0m train(net, trainloader, epoch)\n\u001b[1;32m     10\u001b[0m loss \u001b[39m=\u001b[39m test(net, testloader)\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mclient \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m: centralized model loss \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[13], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, trainloader, epochs, verbose)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      7\u001b[0m     correct, total, epoch_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0.0\u001b[39m\n\u001b[0;32m----> 8\u001b[0m     \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m trainloader:\n\u001b[1;32m     10\u001b[0m         x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(DEVICE), y\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m     11\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m~/anaconda3/envs/FLDL-flwr/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/anaconda3/envs/FLDL-flwr/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/anaconda3/envs/FLDL-flwr/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
            "File \u001b[0;32m~/anaconda3/envs/FLDL-flwr/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
            "File \u001b[0;32m~/anaconda3/envs/FLDL-flwr/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:143\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    140\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/envs/FLDL-flwr/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:143\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    140\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/envs/FLDL-flwr/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:120\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 120\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    122\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    123\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
            "File \u001b[0;32m~/anaconda3/envs/FLDL-flwr/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:172\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m np_str_obj_array_pattern\u001b[39m.\u001b[39msearch(elem\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mstr) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem\u001b[39m.\u001b[39mdtype))\n\u001b[0;32m--> 172\u001b[0m \u001b[39mreturn\u001b[39;00m collate([torch\u001b[39m.\u001b[39mas_tensor(b) \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map)\n",
            "File \u001b[0;32m~/anaconda3/envs/FLDL-flwr/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:172\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m np_str_obj_array_pattern\u001b[39m.\u001b[39msearch(elem\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mstr) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem\u001b[39m.\u001b[39mdtype))\n\u001b[0;32m--> 172\u001b[0m \u001b[39mreturn\u001b[39;00m collate([torch\u001b[39m.\u001b[39;49mas_tensor(b) \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "net = DNN().to(DEVICE)\n",
        "client_n = 30\n",
        "MSE_cen = [0 for i in range(client_n)]\n",
        "epoch = 300\n",
        "\n",
        "for i in range(client_n):\n",
        "  trainloader = trainloaders[i]\n",
        "  testloader = testloaders[i]\n",
        "  train(net, trainloader, epoch)\n",
        "  loss = test(net, testloader)\n",
        "  print(f\"client {i}: centralized model loss {loss}\")\n",
        "  MSE_cen[i] = loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVJaWgOLXgpe",
        "outputId": "42d52f1e-942c-4d74-9fed-b4744a519679"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: validation loss 0.004740860300121185\n",
            "Epoch 2: validation loss 0.004740860321881058\n",
            "Epoch 3: validation loss 0.0047408603291343495\n",
            "Epoch 4: validation loss 0.004740860362499488\n",
            "Epoch 5: validation loss 0.004740860347992906\n",
            "Final test set performance:\n",
            "\tloss 0.003996639648841178\n"
          ]
        }
      ],
      "source": [
        "trainloader = trainloaders[0]\n",
        "valloader = valloaders[0]\n",
        "testloader = testloaders[0]\n",
        "\n",
        "\n",
        "net = DNN().to(DEVICE)\n",
        "\n",
        "for epoch in range(100):\n",
        "    train(net, trainloader, 300)\n",
        "    loss = test(net, valloader)\n",
        "    print(f\"Epoch {epoch+1}: validation loss {loss}\")\n",
        "\n",
        "loss = test(net, testloader)\n",
        "print(f\"Final test set performance:\\n\\tloss {loss}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "FLDL-flwr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "3bee15edd045457655ceb50272c6f515afbaa2aa3595815bf5710b78ef3a3088"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
